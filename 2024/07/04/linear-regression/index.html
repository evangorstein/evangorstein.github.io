<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.126.1">


<title>Linear Regression - Evan Gorstein</title>
<meta property="og:title" content="Linear Regression - Evan Gorstein">


  <link href='//localhost:4321/favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/uwlogo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/">Blog</a></li>
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/evangorstein">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">Linear Regression</h1>

    
    <span class="article-date">2024-07-04</span>
    

    <div class="article-content">
      


<p><font size="1.5"></p>
<p>I’m going back to the basics to discuss something that has confused me for a while about the “error term” in linear regression in the hopes that it might be helpful to someone out there who finds this confusing as well.</p>
<p>Let’s consider the simple case with just a single predictor and suppose we have observed pairs <span class="math inline">\(\{(x_i, y_i), i=1,\ldots,n\}\)</span>. When we do linear regression, we view each data pair <span class="math inline">\((x_i, y_i)\)</span> as the partially observed realization of a triplet of random variables <span class="math inline">\((X_i, Y_i, \varepsilon_i)\)</span> satisfying the equation</p>
<span class="math display" id="eq:reg-eq">\[\begin{equation}
  Y_i = \beta_0 + \beta_1X_i + \varepsilon_i
  
  \tag{1}
\end{equation}\]</span>
<p>for some fixed but unknown constants <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Although this may seem like a model, these equations on their own do not impose any restrictions on our data. After all, algebra allows us to rearrange the terms appearing in an equation, so that <a href="#eq:reg-eq">(1)</a> is equivalent to</p>
<span class="math display" id="eq:reg-eq2">\[\begin{equation}
  \varepsilon_i = Y_i - (\beta_0 + \beta_1X_i).
  
  \tag{2}
\end{equation}\]</span>
<p>Now what’s stopping us from choosing any arbitrary <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and taking <a href="#eq:reg-eq2">(2)</a> to be a definition of <span class="math inline">\(\varepsilon_i\)</span>. From this, we see that Equation <a href="#eq:reg-eq">(1)</a> is trivially satisfied unless we say something additional about <span class="math inline">\(\varepsilon_i\)</span>.</p>
<div id="uncorrelated-with-predictor" class="section level1">
<h1>Uncorrelated with predictor</h1>
<p>One thing we could say about <span class="math inline">\(\varepsilon_i\)</span> to give Equation <a href="#eq:reg-eq">(1)</a> some teeth is that <span class="math inline">\(\operatorname{Cor}(X_i, \varepsilon_i) = 0\)</span>. Let’s take a closer look at what this condition implies in tandem with Equation <a href="#eq:reg-eq">(1)</a>.</p>
<p>If we were to proceed as before and select arbitrary <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, then taking Equation <a href="#eq:reg-eq">(1)</a> as a definition of <span class="math inline">\(\varepsilon_i\)</span> will almost certainly result in an <span class="math inline">\(\varepsilon_i\)</span> that is correlated with <span class="math inline">\(X_i\)</span>. However, there will always be a particular choice of <span class="math inline">\(\beta_1\)</span> such that the satisfaction of Equation <a href="#eq:reg-eq">(1)</a> actually guarantees that <span class="math inline">\(\operatorname{Cor}(X_i, \varepsilon_i) = 0\)</span>, irrespective of the distribution of <span class="math inline">\((X_i, Y_i)\)</span>.</p>
<p>To see this, note that for any <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the satisfaction of Equation <a href="#eq:reg-eq">(1)</a> implies that</p>
<span class="math display" id="eq:uncorr">\[\begin{equation}
\begin{split}
\operatorname{Cov}(X_i, \varepsilon_i) &amp;= \operatorname{Cov}(X_i, Y_i - (\beta_0 + \beta_1X_i)) \\
&amp;= \operatorname{Cov}(X_i, Y_i) - \beta_1\operatorname{Var}(X_i),
\end{split}
\tag{3}
\end{equation}\]</span>
<p>from which its clear that the choice <span class="math inline">\(\beta_1 = \operatorname{Cov}(X_i, Y_i)/\operatorname{Var}(X_i)\)</span> (and only this choice) will lead to <span class="math inline">\(\operatorname{Cov}(X_i, \varepsilon_i) = 0\)</span>. Of course, for this to work, we’ll need <span class="math inline">\(\operatorname{Cov}(X_i, Y_i)/\operatorname{Var}(X_i)\)</span> to not depend on <span class="math inline">\(i\)</span>, that is, to be the same for each data pair. If we assume that the <span class="math inline">\((X_i, Y_i)\)</span> pairs are independently and identically distributed, then this follows.</p>
<p>So we can think of the equation <span class="math inline">\(\operatorname{Cor}(X, \varepsilon) = 0\)</span> as an <em>identifying condition.</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Without this condition, <span class="math inline">\(\beta_1\)</span> could be anything; by imposing <span class="math inline">\(\operatorname{Cor}(X, \varepsilon) = 0\)</span>, we <em>identify</em> <span class="math inline">\(\beta_1\)</span> with <span class="math inline">\(\operatorname{Cov}(X, Y)/\operatorname{Var}(X)\)</span>. Moreover, the condition <span class="math inline">\(\operatorname{Cor}(X, \varepsilon) = 0\)</span> imposes no restrictions on the distribution of our data, as long as we accept this identification of <span class="math inline">\(\beta_1\)</span> with <span class="math inline">\(\operatorname{Cov}(X, Y)/\operatorname{Var}(X)\)</span>.</p>
</div>
<div id="what-is-operatornamecovxyoperatornamevarx" class="section level1">
<h1>What is <span class="math inline">\(\operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span>?</h1>
<p>Continuing without making any assumptions about our data, let’s say we want to predict <span class="math inline">\(Y\)</span> with a linear function of <span class="math inline">\(X\)</span>: <span class="math display">\[
Y \approx b_0 + b_1X
\]</span></p>
<p>A criterion for evaluating our approximating function might be the mean squared error, <span class="math inline">\(\operatorname{MSE}(b_0, b_1) = \operatorname{E}[(Y-b_0 - b_1X))^2]\)</span>. By setting the derivatives of the MSE with respect to <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> equal to <span class="math inline">\(0\)</span>, you can show that <span class="math inline">\(\operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span> is the value of the <span class="math inline">\(b_1\)</span> in the tuple <span class="math inline">\((b_0, b_1)\)</span> that minimizes this criterion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> For this reason, <span class="math inline">\(\operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span> might be a quantity we’re interested in.</p>
<p>In terms of estimation, the ordinary least squares (OLS) estimator takes a plug-in form as the ratio of the <em>sample</em> covariance and <em>sample</em> variance, i.e. <span class="math inline">\(\hat{\beta_1} = \widehat{\operatorname{Cov}(X, Y)}/\widehat{\operatorname{Var}(X)}\)</span>. This statistic will converge in large samples to <span class="math inline">\(\beta_1 = \operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span> in this assumption-less setting.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> We can also apply the Central Limit Theorem with no further assumptions to show that <span class="math inline">\(\hat{\beta_1}\)</span> is asymptotically normal. By estimating its variance, we can then obtain approximate confidence intervals for and conduct hypothesis tests about <span class="math inline">\(\beta_1 = \operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span>, so long as we have a sufficiently large sample. If you’re curious about the details, I recommend taking a look at Sections 5.2 and 7.1-2 of the online textbook <a href="https://mattblackwell.github.io/gov2002-book/">A User’s Guide to Statistical Inference and Regression</a>.</p>
<div id="mean-independent-of-predictor" class="section level2">
<h2>Mean independent of predictor</h2>
<p>Another possible thing we could say about <span class="math inline">\(\varepsilon\)</span> to give Equation <a href="#eq:reg-eq">(1)</a> teeth is that it’s mean independent of <span class="math inline">\(X\)</span>, i.e. <span class="math inline">\(\operatorname{E}[\varepsilon \mid X] = \operatorname{E}[\varepsilon] = 0\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> First note that this assumption actually implies the first one since <span class="math inline">\(\operatorname{Cov}(\varepsilon, X) = \operatorname{E}[X\varepsilon] = \operatorname{E}[X\operatorname{E}[\varepsilon \mid X]] = \operatorname{E}[X*0] = 0\)</span>. Therefore, this assumption also identifies <span class="math inline">\(\beta_1\)</span> with <span class="math inline">\(\operatorname{Cov}(X, Y)/\operatorname{Var}(X)\)</span>. However, this assumption is not <em>only</em> an identifying condition, since it also imposes real restrictions on the distribution of our data. Specifically, it implies that the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is actually linear in <span class="math inline">\(X\)</span>. I would venture to say that most conditional mean functions aren’t actually linear in the collected predictors, so mean independence should be viewed as a somewhat näive assumption.</p>
<p>If mean independence does hold, our estimand <span class="math inline">\(\beta_1 = \operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span> now equals the slope of the linear function that exactly represents the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, not just the linear function that best approximates the conditional mean function, as it was before. In terms of estimation, under the mean independence assumption, the OLS estimator is no longer just consistent, but is in fact unbiased in finite samples for <span class="math inline">\(\beta_1 = \operatorname{Cov}(X,Y)/\operatorname{Var}(X)\)</span>. But again, we can’t typically expect our conditional mean function to be linear, so we can’t typically expect this guarantee.</p>
</div>
<div id="fixed-predictor" class="section level2">
<h2>Fixed predictor</h2>
<p>In discussions of regression in my statistics classes, the values of the predictors were always viewed as fixed or conditioned upon for the purpose of defining the model, so that the only randomness in the model came from the <span class="math inline">\(\varepsilon_i\)</span>’s. The trouble with this perspective is that you lose the ability to have the discussion we just had about what OLS estimates when you don’t make any assumptions other than the fact that you’ve collected a random sample. The MSE criterion depends on the distribution of <span class="math inline">\(X_i\)</span>, so without the distribution of <span class="math inline">\(X_i\)</span> in the model, the fact that the OLS estimator converges to the slope of the linear predictor that minimizes the MSE is meaningless. Thus, from a fixed design perspective, the only way to make statistical sense of what OLS gives you is to make the mean independence assumption, or equivalently, to assume that the conditional mean of <span class="math inline">\(Y\)</span> is actually linear in <span class="math inline">\(X\)</span>.</p>
</div>
<div id="final-note" class="section level2">
<h2>Final note</h2>
<p>So linear regression can be a useful tool even when the linear model doesn’t actually hold, the reason being that <span class="math inline">\(\operatorname{Cov}(X, Y)/\operatorname{Cov}(X)\)</span> is the minimizer among all linear predictors of the MSE, and OLS will automatically asymptotically estimate <span class="math inline">\(\operatorname{Cov}(X, Y)/\operatorname{Cov}(X)\)</span>. This is a fact that has implications beyond just those discussed here. One example is that when you conduct a randomized experiment to estimate a treatment effect, you don’t necessarilly need to worry about violations of linearity when you add baseline covariates to a linear regression model for the outcome. To see why this is, notice that even if your model is wrong, OLS will still automatically (asymptotically) target <span class="math inline">\(\operatorname{Var}(X)^{-1}\operatorname{Cov}(X, Y)\)</span>, where <span class="math inline">\(X\)</span> is now a vector, one of whose components is the treatment variable <span class="math inline">\(A\)</span>, and <span class="math inline">\(Var(X)\)</span> is its variance-covariance matrix. Since the treatment is randomized, it’s uncorrelated with the rest of the components of <span class="math inline">\(X\)</span>; <span class="math inline">\(\operatorname{Var}(X)^{-1}\)</span> will therefore have a block diagonal structure, and the component of <span class="math inline">\(\operatorname{Var}(X)^{-1}\operatorname{Cov}(X, Y)\)</span> that corresponds to the coefficient on <span class="math inline">\(A\)</span> will just be <span class="math inline">\(\operatorname{Cov}(A, Y)/\operatorname{Var}(A)\)</span>, the coefficient from the simple linear regression model that includes only <span class="math inline">\(A\)</span>. Because <span class="math inline">\(A\)</span> is binary, this latter model is saturated, hence correctly specified, and therefore <span class="math inline">\(\operatorname{Cov}(A, Y)/\operatorname{Var}(A)\)</span> is the average treatment effect. So even though the model that includes the baseline covariates is wrong, the coefficient on <span class="math inline">\(A\)</span> in the OLS estimate from this model is not asymptotically biased for the treatment effect. Given the gains in precision that come from including these baseline covariates in the model, one should therefore default to including these baseline covariates in linear regression models for RCT data. For more on this, check out <span class="citation">Wang, Ogburn, and Rosenblum (<a href="#ref-wang2019">2019</a>)</span>.</p>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-wang2019" class="csl-entry">
Wang, Bingkai, Elizabeth L. Ogburn, and Michael Rosenblum. 2019. <span>“Analysis of Covariance in Randomized Trials: More Precision and Valid Confidence Intervals, Without Model Assumptions.”</span> <em>Biometrics</em> 75 (4): 1391–1400. <a href="https://doi.org/10.1111/biom.13062">https://doi.org/10.1111/biom.13062</a>.
</div>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>I’ve dropped subscripts because we’re now assuming the <span class="math inline">\((X_i, Y_i)\)</span>’s are iid from a single distribution <span class="math inline">\(P_{(X, Y)}\)</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The derivative passes through the expectation operator<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Admittedly, we have assumed that <span class="math inline">\((X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)\)</span> are independent and identically distributed, i.e. that they are a random sample from a population, which might be seen as a strong assumption in its own right.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Setting the intercept to be <span class="math inline">\(\beta_0 = \operatorname{E}[Y] - \beta_1\operatorname{E}[X]\)</span> ensures that <span class="math inline">\(\operatorname{E}[\varepsilon] = 0\)</span><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  


  </body>
</html>

